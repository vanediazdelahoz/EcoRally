#!/usr/bin/env python3
import sys
import os
from pathlib import Path
import time
from collections import deque

# Agregar el directorio padre al path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from agent.dynaq_agent import DynaQAgent
from states.board_game import BoardGame

def show_progress_bar(current, total, prefix='Progreso', suffix='Completo', length=50):
    # Barra de progreso
    percent = (current / total) * 100
    filled_length = int(length * current // total)
    bar = '‚ñà' * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent:.1f}% {suffix}', end='', flush=True)
    if current == total:
        print()

class TrainingReporter:
    # Reportes detallados del entrenamiento
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.recent_wins = deque(maxlen=100)
        
    def update(self, episode, won, agent):
        self.recent_wins.append(1 if won else 0)
    
    def generate_report(self, agent, total_episodes, total_time, total_wins):
        print(f"\n{'='*80}")
        print(f"üìä REPORTE COMPLETO DE ENTRENAMIENTO")
        print(f"{'='*80}")
        
        # Estad√≠sticas generales
        win_rate = (total_wins / total_episodes) * 100
        eps_per_sec = total_episodes / total_time
        
        print(f"üéØ ESTAD√çSTICAS GENERALES:")
        print(f"   ‚îú‚îÄ Episodios totales: {total_episodes:,}")
        print(f"   ‚îú‚îÄ Tiempo total: {total_time:.1f} segundos ({total_time/60:.1f} minutos)")
        print(f"   ‚îú‚îÄ Velocidad: {eps_per_sec:.1f} episodios/segundo")
        print(f"   ‚îú‚îÄ Victorias totales: {total_wins:,}")
        print(f"   ‚îî‚îÄ Tasa de victoria: {win_rate:.2f}%")
        
        # Estad√≠sticas del agente
        print(f"\nüß† PAR√ÅMETROS DEL AGENTE:")
        print(f"   ‚îú‚îÄ Alpha (aprendizaje): {agent.alpha}")
        print(f"   ‚îú‚îÄ Gamma (descuento): {agent.gamma}")
        print(f"   ‚îú‚îÄ Epsilon inicial: 0.4")
        print(f"   ‚îú‚îÄ Epsilon actual: {agent.epsilon:.4f}")
        print(f"   ‚îú‚îÄ Epsilon m√≠nimo: {agent.min_epsilon}")
        print(f"   ‚îú‚îÄ Decaimiento epsilon: {agent.epsilon_decay}")
        print(f"   ‚îî‚îÄ Pasos de planificaci√≥n: {agent.planning_steps}")
        
        # An√°lisis del aprendizaje
        print(f"\nüìà AN√ÅLISIS DEL APRENDIZAJE:")
        print(f"   ‚îú‚îÄ Estados totales aprendidos: {len(agent.Q):,}")
        print(f"   ‚îú‚îÄ Transiciones en modelo: {sum(len(actions) for actions in agent.model.values()):,}")
        
        # An√°lisis de rendimiento reciente
        if len(self.recent_wins) >= 50:
            recent_50 = list(self.recent_wins)[-50:]
            recent_win_rate = sum(recent_50) / len(recent_50) * 100
            print(f"   ‚îú‚îÄ Rendimiento √∫ltimas 50 partidas: {recent_win_rate:.1f}%")
            
            if len(self.recent_wins) >= 100:
                recent_100 = list(self.recent_wins)
                recent_win_rate_100 = sum(recent_100) / len(recent_100) * 100
                print(f"   ‚îî‚îÄ Rendimiento √∫ltimas 100 partidas: {recent_win_rate_100:.1f}%")
        
        # Evaluaci√≥n del modelo
        print(f"\nüéñÔ∏è  EVALUACI√ìN DEL MODELO:")
        if win_rate >= 70:
            print(f"   üèÜ EXCELENTE: El agente tiene un rendimiento superior")
        elif win_rate >= 60:
            print(f"   ü•à MUY BUENO: El agente muestra buen aprendizaje")
        elif win_rate >= 50:
            print(f"   ü•â BUENO: El agente est√° aprendiendo efectivamente")
        elif win_rate >= 40:
            print(f"   ‚ö†Ô∏è  REGULAR: El agente necesita m√°s entrenamiento")
        else:
            print(f"   ‚ùå DEFICIENTE: Revisar par√°metros de entrenamiento")
        
        print(f"{'='*80}")
        
        return {
            'win_rate': win_rate,
            'total_episodes': total_episodes,
            'states_learned': len(agent.Q),
            'epsilon_final': agent.epsilon,
            'training_time': total_time
        }

def show_training_menu():
    # Men√∫ de configuraci√≥n de entrenamiento
    print(f"\nü§ñ ENTRENAMIENTO DEL AGENTE")
    print(f"{'='*50}")
    print(f"1. Entrenamiento r√°pido (500 episodios)")
    print(f"2. Entrenamiento est√°ndar (1500 episodios)")
    print(f"3. Entrenamiento intensivo (3000 episodios)")
    print(f"4. Configuraci√≥n personalizada")
    print(f"5. Solo evaluar agente existente")
    print(f"6. Ver informaci√≥n del modelo")
    print(f"7. Volver al men√∫ principal")
    print(f"{'='*50}")

def get_custom_config():
    # Obtiene configuraci√≥n personalizada
    print(f"\n‚öôÔ∏è  CONFIGURACI√ìN PERSONALIZADA")
    print(f"{'='*40}")
    
    while True:
        try:
            episodes = int(input("N√∫mero de episodios (100-10000): "))
            if 100 <= episodes <= 10000:
                break
            else:
                print("‚ùå Debe estar entre 100 y 10000")
        except ValueError:
            print("‚ùå Por favor ingresa un n√∫mero v√°lido")
    
    while True:
        try:
            save_interval = int(input(f"Guardar cada cu√°ntos episodios (10-{episodes//2}): "))
            if 10 <= save_interval <= episodes//2:
                break
            else:
                print(f"‚ùå Debe estar entre 10 y {episodes//2}")
        except ValueError:
            print("‚ùå Por favor ingresa un n√∫mero v√°lido")
    
    return episodes, save_interval

def train_agent(episodes=1500, save_interval=100):
    # Entrena el agente DynaQ usando la l√≥gica exacta de board_game.py
    model_path = "agent/agent_policy.pkl"
    
    print(f"\nüöÄ INICIANDO ENTRENAMIENTO DEL AGENTE")
    print(f"{'='*60}")
    print(f"Episodios: {episodes:,}")
    print(f"Intervalo de guardado: cada {save_interval} episodios")
    print(f"Archivo del modelo: {model_path}")
    print(f"Oponente: Jugador aleatorio (Lulo)")
    print(f"{'='*60}\n")
    
    # Crear directorio agent si no existe
    os.makedirs("agent", exist_ok=True)
    
    # Par√°metros optimizados para mejor rendimiento
    agent = DynaQAgent(
        train_mode=True,
        alpha=0.2,           # Tasa de aprendizaje
        gamma=0.99,          # Factor de descuento (visi√≥n a largo plazo)
        epsilon=0.4,         # Exploraci√≥n inicial
        planning_steps=15    # Pasos de planificaci√≥n
    )
    
    agent.min_epsilon = 0.01
    agent.epsilon_decay = 0.9998
    
    print(f"üß† PAR√ÅMETROS OPTIMIZADOS:")
    print(f"   ‚îú‚îÄ Alpha (aprendizaje): {agent.alpha}")
    print(f"   ‚îú‚îÄ Gamma (descuento): {agent.gamma}")
    print(f"   ‚îú‚îÄ Epsilon inicial: {agent.epsilon}")
    print(f"   ‚îú‚îÄ Epsilon m√≠nimo: {agent.min_epsilon}")
    print(f"   ‚îú‚îÄ Decaimiento epsilon: {agent.epsilon_decay}")
    print(f"   ‚îî‚îÄ Pasos planificaci√≥n: {agent.planning_steps}")
    print()
    
    # Intentar cargar pol√≠tica previa
    if os.path.exists(model_path):
        if agent.load_policy(model_path):
            print("‚úÖ Pol√≠tica previa cargada. Continuando entrenamiento...")
        else:
            print("‚ö†Ô∏è Error al cargar pol√≠tica previa. Iniciando desde cero.")
    else:
        print("üí° No se encontr√≥ pol√≠tica previa. Iniciando entrenamiento desde cero.")
    
    # Inicializar reporter
    reporter = TrainingReporter()
    
    # Estad√≠sticas
    victorias = 0
    start_time = time.time()
    
    print(f"\nüéÆ INICIANDO ENTRENAMIENTO...")
    print(f"ü§ñ El agente entrena contra oponente aleatorio")
    print(f"{'‚îÄ'*60}")
    
    # Entrenar
    for episode in range(episodes):
        # Mostrar progreso cada 1% del total
        if episode % max(1, episodes // 100) == 0 or episode == episodes - 1:
            show_progress_bar(episode + 1, episodes, 'Entrenamiento')
        
        # USAR LA L√ìGICA EXACTA DE BOARD_GAME.PY
        won = BoardGame(use_agent=True, train_agent=True, agent=agent, silent_mode=True)
        
        if won:
            victorias += 1
        
        # Actualizar reporter
        reporter.update(episode, won, agent)
        
        # Guardar peri√≥dicamente
        if (episode + 1) % save_interval == 0:
            agent.save_policy(model_path)
    
    # Asegurar que la barra llegue al 100%
    show_progress_bar(episodes, episodes, 'Entrenamiento')
    
    # Guardar final
    agent.save_policy(model_path)
    total_time = time.time() - start_time
    
    # Generar reporte
    reporter.generate_report(agent, episodes, total_time, victorias)
    
    return agent

def test_agent(episodes=10):
    model_path = "agent/agent_policy.pkl"
    
    print(f"\nüß™ EVALUACI√ìN DEL AGENTE")
    print(f"{'='*40}")
    
    if not os.path.exists(model_path):
        print("‚ùå No se encontr√≥ el modelo entrenado")
        print("üí° Primero debes entrenar el agente")
        return
    
    agent = DynaQAgent(train_mode=False)
    if not agent.load_policy(model_path):
        print("‚ùå Error al cargar el modelo")
        return
    
    print(f"‚úÖ Modelo cargado: {len(agent.Q):,} estados")
    print(f"üéØ Evaluando con {episodes} partidas...")
    print(f"{'‚îÄ'*40}")
    
    victorias = 0
    
    for i in range(episodes):
        # Mostrar progreso de evaluaci√≥n
        show_progress_bar(i + 1, episodes, 'Evaluaci√≥n')
        
        won = BoardGame(use_agent=True, train_agent=False, agent=agent, silent_mode=True)
        
        if won:
            victorias += 1
    
    tasa_victoria = (victorias / episodes) * 100
    
    print(f"\nüìä RESULTADOS:")
    print(f"   ‚îú‚îÄ Victorias: {victorias}/{episodes}")
    print(f"   ‚îî‚îÄ Tasa de victoria: {tasa_victoria:.1f}%")
    
    if tasa_victoria >= 70:
        print(f"üåü RENDIMIENTO: EXCELENTE")
    elif tasa_victoria >= 60:
        print(f"ü•á RENDIMIENTO: MUY BUENO")
    elif tasa_victoria >= 50:
        print(f"ü•à RENDIMIENTO: BUENO")
    else:
        print(f"‚ö†Ô∏è RENDIMIENTO: NECESITA MEJORA")

def show_model_info():
    """Muestra informaci√≥n del modelo"""
    model_path = "agent/agent_policy.pkl"
    
    print(f"\nüìä INFORMACI√ìN DEL MODELO")
    print(f"{'='*40}")
    
    if not os.path.exists(model_path):
        print("‚ùå No se encontr√≥ el modelo")
        return
    
    agent = DynaQAgent(train_mode=False)
    if agent.load_policy(model_path):
        print(f"‚úÖ Modelo cargado correctamente")
        print(f"üìà Estados aprendidos: {len(agent.Q):,}")
        print(f"üìÅ Tama√±o del archivo: {os.path.getsize(model_path):,} bytes")
        print(f"üß† Par√°metros:")
        print(f"   ‚îú‚îÄ Alpha: {agent.alpha}")
        print(f"   ‚îú‚îÄ Gamma: {agent.gamma}")
        print(f"   ‚îú‚îÄ Epsilon: {agent.epsilon}")
        print(f"   ‚îî‚îÄ Planning steps: {agent.planning_steps}")
    else:
        print("‚ùå Error al cargar el modelo")

def main():
    """Funci√≥n principal con men√∫ interactivo"""
    while True:
        show_training_menu()
        
        try:
            choice = input("\nSelecciona una opci√≥n (1-7): ").strip()
            
            if choice == "1":
                print("\n‚ö° ENTRENAMIENTO R√ÅPIDO")
                train_agent(episodes=500, save_interval=50)
                
            elif choice == "2":
                print("\nüìà ENTRENAMIENTO EST√ÅNDAR")
                train_agent(episodes=1500, save_interval=100)
                
            elif choice == "3":
                print("\nüéØ ENTRENAMIENTO INTENSIVO")
                train_agent(episodes=3000, save_interval=150)
                
            elif choice == "4":
                episodes, save_interval = get_custom_config()
                print(f"\n‚öôÔ∏è  ENTRENAMIENTO PERSONALIZADO")
                train_agent(episodes=episodes, save_interval=save_interval)
                
            elif choice == "5":
                test_agent(episodes=10)
                
            elif choice == "6":
                show_model_info()
                
            elif choice == "7":
                print("\nüëã Volviendo al men√∫ principal...")
                break
                
            else:
                print("‚ùå Opci√≥n inv√°lida")
                
            # Preguntar si quiere continuar despu√©s del entrenamiento
            if choice in ["1", "2", "3", "4"]:
                print(f"\n¬øQuieres evaluar el agente entrenado? (s/n): ", end="")
                response = input().strip().lower()
                if response.startswith('s'):
                    test_agent()
                    
        except KeyboardInterrupt:
            print(f"\n\n‚èπÔ∏è  Operaci√≥n cancelada")
            break
        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()